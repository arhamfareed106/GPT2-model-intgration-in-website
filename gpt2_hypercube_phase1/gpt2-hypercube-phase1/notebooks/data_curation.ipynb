{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data curation notebook\n",
        "Minimal pipeline: load raw MAG items, filter by quality/toxicity/length, deduplicate, scrub PII, format training pairs and save curated JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import random\n",
        "from services.privacy import scrub_record\n",
        "\n",
        "# Compute repo root assuming notebook lives in /notebooks\n",
        "ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "RAW_PATH = os.path.join(ROOT, 'data', 'raw_mag.json')  # replace with your raw MAG dump\n",
        "OUT_PATH = os.path.join(ROOT, 'data', 'curated_training.json')\n",
        "\n",
        "print('ROOT:', ROOT)\n",
        "print('RAW_PATH:', RAW_PATH)\n",
        "print('OUT_PATH:', OUT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw items (placeholder if file missing)\n",
        "if os.path.exists(RAW_PATH):\n",
        "    with open(RAW_PATH, 'r', encoding='utf-8') as f:\n",
        "        raw = json.load(f)\n",
        "else:\n",
        "    print('RAW file not found; creating toy examples')\n",
        "    raw = [\n",
        "        {\"id\": \"1\", \"text\": \"How to compute NPV?\", \"response\": \"Here is how you compute NPV step-by-step...\", \"importance\": 0.9, \"timestamp\": \"2025-10-25T12:00:00Z\", \"quality\": 0.95},\n",
        "        {\"id\": \"2\", \"text\": \"What's my SSN 123-45-6789?\", \"response\": \"I can't help with that.\", \"importance\": 0.1, \"timestamp\": \"2025-01-01T00:00:00Z\", \"quality\": 0.1}\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_high_quality(item, min_len=15, min_quality=0.5):\n",
        "    txt = (item.get('text') or '') + ' ' + (item.get('response') or '')\n",
        "    if len(txt.strip()) < min_len:\n",
        "        return False\n",
        "    if item.get('quality') is not None and item.get('quality') < min_quality:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def is_toxic(item):\n",
        "    # placeholder: integrate Detoxify, Perspective API or your classifier here\n",
        "    txt = (item.get('text') or '') + ' ' + (item.get('response') or '')\n",
        "    tox_triggers = ['kill', 'suicide', 'hate', 'slur']\n",
        "    t = txt.lower()\n",
        "    return any(w in t for w in tox_triggers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filtering pipeline\n",
        "filtered = []\n",
        "seen = set()\n",
        "for it in raw:\n",
        "    if not is_high_quality(it):\n",
        "        continue\n",
        "    if is_toxic(it):\n",
        "        continue\n",
        "    # privacy scrub\n",
        "    scrubbed = scrub_record({'input': it.get('text',''), 'target': it.get('response','')}, fields=('input','target'), replace_with='[REDACTED]', use_spacy=False)\n",
        "    # dedupe by normalized input+target\n",
        "    key = (scrubbed['input'].strip().lower(), scrubbed['target'].strip().lower())\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    formatted = {\n",
        "        'input': '<|persona:analyst|>\\nUser: ' + scrubbed['input'].strip() + '\\nAssistant:',\n",
        "        'target': ' ' + scrubbed['target'].strip(),\n",
        "        'source_id': it.get('id')\n",
        "    }\n",
        "    filtered.append(formatted)\n",
        "\n",
        "print('Filtered count:', len(filtered))\n",
        "if filtered:\n",
        "    print('Example:', filtered[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: prioritize recent & important items (toy scorer)\n",
        "def score_item_raw(raw_item):\n",
        "    importance = raw_item.get('importance', 0.5)\n",
        "    ts = raw_item.get('timestamp')\n",
        "    recency = 0.0\n",
        "    try:\n",
        "        rec_dt = datetime.fromisoformat(ts.replace('Z','+00:00'))\n",
        "        days = (datetime.utcnow() - rec_dt).days\n",
        "        recency = max(0, 1 - days/365)\n",
        "    except Exception:\n",
        "        recency = 0.0\n",
        "    return 0.7 * importance + 0.3 * recency\n",
        "\n",
        "# here we shuffled; replace with scoring/importance sorting if you keep original raw items\n",
        "filtered_sorted = sorted(filtered, key=lambda x: random.random())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save curated dataset\n",
        "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
        "with open(OUT_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(filtered_sorted, f, ensure_ascii=False, indent=2)\n",
        "print('Saved curated dataset to', OUT_PATH)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}